%************************************************
\chapter{General Discussion and Conclusions}\label{ch:discusion}
%************************************************
\section{General Discussion}
The different contributions that make up this thesis have already been discussed in detail at each chapter. However, in this last chapter, we will discuss the contributions that this work makes to neuroimaging and the \ac{CAD} field itself, and what discoveries (or confirmations) we have done on the different diseases that have been analysed. 

\subsection{Discussion on the algorithms}
As commented at the introduction, in this thesis we proposed different strategies for tackling the small sample size problem. The first three approaches (chapters~\ref{ch:decomposition}, \ref{ch:texture} and \ref{ch:sbm}) are feature extraction algorithms that perform a significant reduction of the number of features used in neuroimaging. 

% Decomposition 
In the decomposition approach (chapter~\ref{ch:decomposition}), we obtained a very significant feature reduction, from hundreds of thousands of voxels to between 2 and 25 features, which were the coordinates of each sample in the space defined by the components. Both \ac{FA} and \ac{ICA} were able to detect similar regions in the \ac{AD} and \ac{PKS} functional datasets, obtaining an accuracy higher than 90\% in the first and higher than 95\% in the latter. 

The decomposition also makes our \ac{CAD} better generalizable, since the features are no longer subject to the small sample size problem (the number of subjects is several times higher than the number of features). The samples are projected to a dense space, where the \ac{SVC} are able to perform a reliable classification, and it also implies a more accurate model of the diseases studied. 

Thanks to the softness of nuclear imaging techniques such as \ac{PET} and \ac{SPECT}, the decomposition techniques are a very useful tool to characterize and predict the stage of a disease. However, due to their resolution, they could not perform as well in structural \ac{MRI} datasets unless they have been previously smoothed. 

% Texture
As for the texture-based \ac{CAD} proposed in Chapter~\ref{ch:texture}, we have already seen its potential when applied to DaTSCAN images. In these images, the cluster tendency and homogeneity were the texture features that achieved best performance, which is closely related to the characteristics of the images. Since the distribution of intensities is highly concentrated around the striatum, differences in shape (cluster tendency) and in the distribution of the radiopharmaceutical (homogeneity) are usually regarded by physicians when working with this modality. A combination of these and other texture features yielded the best outcome, achieving up to 97\% accuracy in the PPMI-DAT dataset (excluding \ac{SWEDD} subjects) -COMPROBAR-, when using the relative entropy (or \ac{KL} divergence) as selection criterion. 

Despite their ability in detecting \ac{PD} related \ac{DAT} deficit in DaTSCAN imaging, these texture features are an inviting possibility to explore in conjunction with other modalities such as \ac{MRI}. In structural images, textural information in high resolution is available, which could be exploited to characterize textural changes in longitudinal datasets, and associate these with the progression of neurodegeneration. 

% SBM
Finally, we have proposed \ac{SBM} in Chapter~\ref{ch:sbm}, a novel technique that maps structural (and possibly functional) images to two-dimensional maps representing various measures. The \ac{SBM} establishes a framework that has been expanded witch divisions on the mapping vector, extensions to the type of sampling and even a path tracing algorithm based on \ac{HMM}. Of these, the most powerful approaches were the original \ac{SBM} measures (specifically, the average of the the intensities selected by the mapping vector) and the \ac{VRLBP} approach, that characterized the texture of and around the mapping vector by means of an helical sampling. The \ac{HMM} paths, for their part, were not as powerful as a feature selection algorithm, although their ability to adapt to the intensity changes on the images make them a perfect candidate for testing new possibilities such as morphological measures, or even image segmentation. 

The whole \ac{SBM} framework is still to be developed, but it shows very promising results in the classification of structural images. In some preliminary results that we are currently testing, the application of \ac{SBM} to predict the conversion of \ac{MCI} to dementia versus \ac{MCI} stable has reached the nondescript amount of 77.6\% accuracy. The \ac{MCI} conversion is today a major challenge, and results like these open a whole range of possibilities. 

% SWPCA
On the other hand, chapters~\ref{ch:swpca} and \ref{ch:simulation} aimed at increasing the number of samples available, and solve problems that usually appear when working with large datasets. The \ac{SWPCA} (Chapter~\ref{ch:swpca}), developed by the PhD candidate at the University of Cambridge, was intended to correct many inhomogeneities that have been identified in multicentre datasets like the \aimsmri{}. These inhomogeneities caused a \ac{SVC} to be able to distinguish between centres with higher accuracy than between subjects affected by \ac{ASD}. Our intention was to correct this behaviour by decomposing the images using \ac{PCA} and parsing out the components whose contribution was more related to the acquisition site. After applying \ac{SWPCA} to the dataset we found that many recent claims about \ac{ASD} heterogeneity \cite{haar2014anatomical} were well founded, given that the differences between individuals affected and not-affected by the disorder were almost null. This makes it a very useful tool for merging structural dataset acquired at different centres, with the reported limitation that it needs a large sample size at each centre to work out these site differences. 

% Synthesis. 
Finally, the functional brain synthesis algorithm proposed in Chapter~\ref{ch:simulation} offers an inviting possibility to every neuroimaging scientist: generate hundreds of new images that share characteristics with a certain datasets. It only needs a large enough dataset (of hundreds of images) that has been previously normalized in both space and intensity. Our algorithm transform the original dataset to the `eigenbrain' space, where the statistical distribution of each class is estimated, and then, new coordinates of these can be generated. The synthesized images closely resemble the original ones, and physicians were unable to realize which one was real and which one synthesized in a preliminary test. The experiments proposed in that chapter prove that the synthesized images can effectively predict real world examples and, at the same time, be independent from the original ones. A \ac{SPM} analysis revealed that the significant class differences are in the same location in both original and simulated images. 

We have proved the ability of all these algorithms in the differential diagnosis of two diseases, \ac{AD} and \ac{PD}, and the utility of the \ac{SWPCA} to reduce the site-related inhomogeneities in multicentre datasets. All these algorithms perform either a reduction of the feature space or a safe increase of the sample size in order to reduce the amount of false positives currently found in neuroimaging studies. Other improvements, such as the computational load reduction thanks to feature extraction or the \ac{SBM} visualization tools may also be acknowledged. 


\subsection{Discussion on the diseases}
All the aforementioned \ac{CAD} tools have been applied to different neuroimaging datasets that comprise \ac{AD} (chapters~\ref{ch:decomposition}, \ref{ch:sbm} and \ref{ch:simulation}), \ac{PKS} (chapters~\ref{ch:decomposition}, \ref{ch:texture} and \ref{ch:simulation}) and \ac{ASD} (chapter~\ref{ch:swpca}). Several structural (chapters~\ref{ch:sbm} and \ref{ch:swpca}) and functional (chapter~\ref{ch:decomposition}, \ref{ch:texture} and \ref{ch:simulation}) have been analysed for all these diseases as well. 

% AD
Regarding \ac{AD}, we found consistent hypometabolism and hypoperfusion patterns, using the \adnipet{} and \vdlnhmpao{} datasets. In chapter~\ref{ch:decomposition} we reported general patterns that could be found in both modalities, mainly at the occipital and temporal lobes, with strong focus on the angular gyrus \cite{Dubois2007,Claus1994}. These patterns were afterwards confirmed at ch.~\ref{ch:simulation}. 

Appart from these common regions, the most relevant differences in the \adnipet{} dataset were located at the angular gyrus and the cingulum, although fundamental structures linked to \ac{AD} such as the hippocampus or the parahippocampal gyrus were also highlighted by some of the selection criteria \cite{Stoeckel04,Illan2011}. However, when using the \vdlnhmpao{}, the selected regions were more diffuse (probably due to less resolution images), and mainly located at the angular gyrus, the occipital lobe and parts of the temporal lobe \cite{Dubois2007,Claus1994}. When using these selection criteria (and consequently, the intensity of these regions), our system achieved the best performance, which gives us an idea of the most relevant differences between \ac{AD} and \ac{CTL} in functional datasets. 

\ac{AD} was also analysed at chapter~\ref{ch:sbm}, by means of the \ac{SBM} of \ac{MRI} datasets. We already mentioned the best performing measures: the average and the \ac{VRLBP}, which may be linked to anatomical properties such as tissue density and texture. When overimposing the reference image to the \ac{SBM} $t$-maps over the average measure, fundamental differences were found at the middle temporal lobe, amygdala, hippocampus, parahippocampal gyrus or some structures of the basal ganglia, such as caudate nucleus, globus pallidus or putamen \cite{Dubois2007,Pievani2013}. The best performing \ac{VRLBP} approach, however, was more precise, and focused mainly on small areas at the temporal lobe, and the space between the amygdala and hippocampus. It was in these parts where most of the texture changes were detected, and its higher performance gives a hint about the relevance of this report. 



The average is the best-performing measure in this group, and can be interpreted as the average amount of tissue -even tissue density- in the direction $(\theta,\varphi)$. A comparison between the densities of the tissues in \ac{AD} and \ac{CTL} can reveal the neurodegeneration and tissue loss typically associated with the disease. This is perhaps why in the significance analysis we found high absolute $t$-values in areas typically affected by \ac{AD}, such as the mid temporal lobe, amygdala, hippocampus, parahippocampal lobe or some structures of the basal ganglia, such as caudate nucleus, globus pallidus or putamen \cite{Dubois2007,Pievani2013}. 

% PKS

% ASD

%\section{Future Work}

\section{Conclusions}


All these algorithms and frameworks are indeed complementary. One can use the synthesis algorithm to double the size of an existing dataset, and then use decomposition to reduce the feature space. Or purge a multi-centre database with \ac{SWPCA} and then analyse and visualize the differences using \ac{SBM}. These are only recent contributions to the neuroimage processing 

