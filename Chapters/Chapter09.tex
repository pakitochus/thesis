%************************************************
\chapter{General Discussion and Conclusions}\label{ch:discusion}
%************************************************
\section{General Discussion}
The different contributions that make up this thesis have already been discussed in detail at each chapter. However, in this last chapter, we will discuss the contributions that this work makes to neuroimaging and the \ac{CAD} field itself, and what discoveries (or confirmations) we have done on the different diseases that have been analysed. 

\subsection{Discussion on the algorithms}
As commented at the introduction, in this thesis we proposed different strategies for tackling the small sample size problem. The first three approaches (chapters~\ref{ch:decomposition}, \ref{ch:texture} and \ref{ch:sbm}) are feature extraction algorithms that perform a significant reduction of the number of features used in neuroimaging. 

% Decomposition 
In the decomposition approach (chapter~\ref{ch:decomposition}), we obtained a very significant feature reduction, from hundreds of thousands of voxels to between 2 and 25 features, which were the coordinates of each sample in the space defined by the components. Both \ac{FA} and \ac{ICA} were able to detect similar regions in the \ac{AD} and \ac{PKS} functional datasets, obtaining an accuracy higher than 90\% in the first and higher than 95\% in the latter. 

The decomposition also makes our \ac{CAD} better generalizable, since the features are no longer subject to the small sample size problem (the number of subjects is several times higher than the number of features). The samples are projected to a dense space, where the \ac{SVC} are able to perform a reliable classification, and it also implies a more accurate model of the diseases studied. 

Thanks to the softness of nuclear imaging techniques such as \ac{PET} and \ac{SPECT}, the decomposition techniques are a very useful tool to characterize and predict the stage of a disease. However, due to their resolution, they could not perform as well in structural \ac{MRI} datasets unless they have been previously smoothed. 

% Texture
As for the texture-based \ac{CAD} proposed in Chapter~\ref{ch:texture}, we have already seen its potential when applied to DaTSCAN images. In these images, the cluster tendency and homogeneity were the texture features that achieved best performance, which is closely related to the characteristics of the images. Since the distribution of intensities is highly concentrated around the striatum, differences in shape (cluster tendency) and in the distribution of the radiopharmaceutical (homogeneity) are usually regarded by physicians when working with this modality. A combination of these and other texture features yielded the best outcome, achieving up to 97\% accuracy in the PPMI-DAT dataset (excluding \ac{SWEDD} subjects) -COMPROBAR-, when using the relative entropy (or \ac{KL} divergence) as selection criterion. 

Despite their ability in detecting \ac{PD} related \ac{DAT} deficit in DaTSCAN imaging, these texture features are an inviting possibility to explore in conjunction with other modalities such as \ac{MRI}. In structural images, textural information in high resolution is available, which could be exploited to characterize textural changes in longitudinal datasets, and associate these with the progression of neurodegeneration. 

% SBM
Finally, we have proposed \ac{SBM} in Chapter~\ref{ch:sbm}, a novel technique that maps structural (and possibly functional) images to two-dimensional maps representing various measures. The \ac{SBM} establishes a framework that has been expanded witch divisions on the mapping vector, extensions to the type of sampling and even a path tracing algorithm based on \ac{HMM}. Of these, the most powerful approaches were the original \ac{SBM} measures (specifically, the average of the the intensities selected by the mapping vector) and the \ac{VRLBP} approach, that characterized the texture of and around the mapping vector by means of an helical sampling. The \ac{HMM} paths, for their part, were not as powerful as a feature selection algorithm, although their ability to adapt to the intensity changes on the images make them a perfect candidate for testing new possibilities such as morphological measures, or even image segmentation. 

The whole \ac{SBM} framework is still to be developed, but it shows very promising results in the classification of structural images. In some preliminary results that we are currently testing, the application of \ac{SBM} to predict the conversion of \ac{MCI} to dementia versus \ac{MCI} stable has reached the nondescript amount of 77.6\% accuracy. The \ac{MCI} conversion is today a major challenge, and results like these open a whole range of possibilities. 

% SWPCA
On the other hand, chapters~\ref{ch:swpca} and \ref{ch:simulation} aimed at increasing the number of samples available, and solve problems that usually appear when working with large datasets. The \ac{SWPCA} (Chapter~\ref{ch:swpca}), developed by the PhD candidate at the University of Cambridge, was intended to correct many inhomogeneities that have been identified in multicentre datasets like the AIMS-MRI. These inhomogeneities caused a \ac{SVC} to be able to distinguish between centres with higher accuracy than between subjects affected by \ac{ASD}. Our intention was to correct this behaviour by decomposing the images using \ac{PCA} and parsing out the components whose contribution was more related to the acquisition site. After applying \ac{SWPCA} to the dataset we found that many recent claims about \ac{ASD} heterogeneity \cite{haar2014anatomical} were well founded, given that the differences between individuals affected and not-affected by the disorder were almost null. This makes it a very useful tool for merging structural dataset acquired at different centres, with the reported limitation that it needs a large sample size at each centre to work out these site differences. 

% Synthesis. 
Finally, the functional brain synthesis algorithm proposed in Chapter~\ref{ch:simulation} offers an inviting possibility to every neuroimaging scientist: generate hundreds of new images that share characteristics with a certain datasets. It only needs a large enough dataset (of hundreds of images) that has been previously normalized in both space and intensity. Our algorithm transform the original dataset to the `eigenbrain' space, where the statistical distribution of each class is estimated, and then, new coordinates of these can be generated. The synthesized images closely resemble the original ones, and physicians were unable to realize which one was real and which one synthesized in a preliminary test. The experiments proposed in that chapter prove that the synthesized images can effectively predict real world examples and, at the same time, be independent from the original ones. A \ac{SPM} analysis revealed that the significant class differences are in the same location in both original and simulated images. 




\subsection{Discussion on the diseases}
In the case of the VDLN-HMPAO dataset, the most interesting regions are located at the occipital lobe, the angular lobe and few of them in the temporal lobe. These are selected using almost any of the selection criteria. However, when using the ADNI-PET dataset, the only region with a significant overlapping is the angular gyrus, and other regions with a widely documented relation to \ac{AD} are highlighted \cite{Dubois2007,Claus1994}, such as the cingulum, hippocampus and parahippocampal gyrus. 

It is clearly noticeable that the relative entropy selection criterion focuses on many different regions, but it is the only one able to detect the hippocampus or parahippocampal gyrus in the \ac{PET} dataset, which other criteria ignore. It also focuses more on the different parts of the occipital lobe in the \ac{SPECT} dataset. This difference in the selected areas could lead to the different overall performance observed in Figures~\ref{fig:AD-AV-FA-ENTROPY-VSN}, \ref{fig:AD-AV-FA-ENTROPY-VSK}, \ref{fig:AD-AV-ICA-ENTROPY-VSN}, and \ref{fig:AD-AV-ICA-ENTROPY-VSK}.

For its part, wilcoxon and $t$-test often select similar regions. This can be due to their similarity under the normal distribution \cite{Fay10}, and leads to a higher performance in the systems in average and at the operation point (see Figures~\ref{fig:accuracyMeanFA-AD}, \ref{fig:accuracyMeanICA-AD}, \ref{fig:accuracyOP-ADvsN} and \ref{fig:accuracyOP-AD}). From the selected regions, and since $t$-test and wilcoxon perform generally better, we can infer the more interesting regions for \ac{AD} classification. For the ADNI-PET dataset, these would be the angular lobes and the cingulum, whereas for the VDLN-HMPAO, we can observe differences in the angular lobe and also all over the occipital lobe and parts of the temporal lobe. 

%\section{Future Work}

\section{Conclusions}


All these algorithms and frameworks are indeed complementary. One can use the synthesis algorithm to double the size of an existing dataset, and then use decomposition to reduce the feature space. Or purge a multi-centre database with \ac{SWPCA} and then analyse and visualize the differences using \ac{SBM}. These are only recent contributions to the neuroimage processing 

