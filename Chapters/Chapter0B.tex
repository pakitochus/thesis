%********************************************************************
% Appendix
%*******************************************************
% If problems with the headers: get headings in appendix etc. right
%\markboth{\spacedlowsmallcaps{Appendix}}{\spacedlowsmallcaps{Appendix}}
\chapter{Background on Support Vector Machines}\label{ch:svm}
\acfp{SVM}, introduced in the late 70s \cite{Vapnik1998}, are a set of related supervised learning methods widely used in classification (\acp{SVC}), regression analysis and, more generally, in any field of machine learning. They have been extensively used in neuroimaging applications such as the ones presented in this thesis \cite{Martinez-Murcia2013255,Martinez-Murcia201458,martinez2014parametrization,Martinez-Murcia2015,Martinez-Murcia2016,Martinez-Murcia2016a} and many others \cite{Stoeckel04,Towey2011,Illan2011,Illan2012,Ortiz2013,Segovia2016a}.

The simplest \ac{SVC} assumes the data to be linearly separable. Each sample of a given dataset is considered to be a $p$-dimensional vector. Therefore, the objective is to separate a set of data, with binary labels, using a hyperplane (multidimensional plane) that maximizes the distance between the two classes. Therefore, we must build a function $f : \Re^p \rightarrow \lbrace \pm1 \rbrace$ that is able to assign a binary value (+1 or -1) to a new sample. This function is built using a set of training data $\mathbf{X}$, which contains $N$ $p$-dimensional vectors (samples) $\mathbf{x}_i$ and their corresponding class $y_i$: 

\begin{equation}
(\mathbf{x_1} , y_1 ), (\mathbf{x_2} , y_2 ), ..., (\mathbf{x_l} , y_l ) \in \Re^p \times \lbrace \pm 1\rbrace
\end{equation}

The linear \ac{SVC} defines a separation hyperplane in a multidimensional space using the following function: 
\begin{equation}
g(\textbf{x}) = \textbf{w}^T \textbf{x} + \omega_0 = 0,
\end{equation}
where \textbf{w} is a weight vector, and $\omega_0$ is the threshold. Finding weight vector orthogonal to the maximum separation hyperplane and the unknown thresholds  $\omega_i , i = 1,...,n$ is known as ``training'' a \ac{SVC}. Our training data can be described therefore as: 
\begin{align}
	\mathbf{x}_i\mathbf{w} + \omega_0 &\ge +1 \quad \text{for} y_i = +1\\
	\mathbf{x}_i\mathbf{w} + \omega_0 &\le -1 \quad \text{for} y_i = -1\\
\end{align}
which can be combined to a simpler equation:
\begin{equation}\label{eq:constraint}
	y_i(\mathbf{x}_i\mathbf{w})-1\ge 0 \quad \forall i
\end{equation}

Maximizing the margin of the hyperplane is equivalent to minimize $||\mathbf{w}||$ subject to Eq.~\ref{eq:constraint}. This can be translated, if we want to use Quadratic Programming, to minimize $\frac{1}{2}||\mathbf{w}||^2$. 

Suppose now that the data is not linearly separable (as it could be expected from real data). In this case, we introduce the slack variable $\xi_i$, and Eq.~\ref{eq:constraint} is converted into: 
\begin{equation}
y_i(\mathbf{x}_i\mathbf{w})-1 + \xi_i \ge 0 \quad \text{where} \quad \xi_i\ge 0 \quad\forall i
\end{equation}

This is known as soft margin \ac{SVM}. In this case, we penalize the data points on the incorrect side of the separation hyperplane. We can adapt then the maximization of the margin with: 
\begin{equation}\label{eq:softSVM}
	\text{minimize} \quad \frac{1}{2}||\mathbf{w}||^2 + C \sum_i^L \xi_i \quad \text{s.t.} \quad y_i(\mathbf{x}_i\mathbf{w})-1\ge 0 \quad \forall i
\end{equation}

This procedure is known as regularization, and parameter $C$ controls the trade-off between the slack variable penalty and the size of the margin.

Throughout this thesis we have used the \ac{SVM} implementation contained in \texttt{LIBLINEAR} \cite{Chang2001} and the \texttt{LinearSVC} contained within \texttt{scikit-learn} in python, which is based upon \texttt{LIBLINEAR} itself. We used the L2-regularized L2-loss \ac{SVC} solver, which summarizes Eq.~\ref{eq:softSVM} in the equation: 

\begin{equation}
\text{minimize}_\mathbf{w}\quad  \frac{\mathbf{w}^T\mathbf{w}}{2} + C \sum \max(0, 1- y_i \mathbf{w}^T\mathbf{x}_i)^2
\end{equation}
where we can see that the regularization parameter controls the sum of squared losses, as it corresponds to the L2 solver. The minimization is performed via the Trust Region Newton method (TRON), described in detail at \cite{Lin2008}.

%% DONE
