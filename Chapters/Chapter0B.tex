%********************************************************************
% Appendix
%*******************************************************
% If problems with the headers: get headings in appendix etc. right
%\markboth{\spacedlowsmallcaps{Appendix}}{\spacedlowsmallcaps{Appendix}}
\chapter{Background on Support Vector Machines}\label{ch:svm}
\acf{SVM}, introduced in the late 70s \cite{Vapnik1982}, are a set of related supervised learning methods widely used in pattern recognition, voice activity detection (VAD), classification and regression analysis.

We suppose the data to be linearly separable. In this case, a data point is viewed as a $p$-dimensional vector. Our objective is to separate a set of binary labelled training data with a hyperplane that is maximally distant from the two classes (known as the maximal margin hyper-plane). To do so, we build a function $f : \Re^n -> \lbrace \pm1 \rbrace$ using training data that is, $p$-dimensional patterns $x_i$ and class labels $y_i$ :

\begin{equation}
(\mathbf{x_1} , \mathbf{y_1} ), (\mathbf{x_2} , \mathbf{y_2} ), ..., (\mathbf{x_l} , \mathbf{y_l} ) \in \Re^n \times \lbrace \pm 1\rbrace
\end{equation}

\noindent so that $f$ will correctly classify new examples ($\mathbf{x}$, $\mathbf{y}$).

Linear discriminant functions define decision hypersurfaces or hyperplanes
in a multidimensional feature space, that is:

\begin{equation}
g(\textbf{x}) = \textbf{w}^T \textbf{x} + \omega_0 = 0,
\end{equation}
where \textbf{w} is known as the weight vector and $\omega_0$ as the threshold. The weight vector \textbf{w} is orthogonal to the decision hyperplane and the optimization task consists of finding the unknown parameters $\omega_i , i = 1,...,n$ defining the decision hyperplane.

Let $\mathbf{x_i}, i=1,2,...,n$ be the feature vectors of the training set, $X$. These belong to either of the two classes, $\omega_1$ or $\omega_2$. If the classes were linearly separable, the objective would be to design a hyperplane that classifies correctly all the training vectors. The hyperplane is not unique, and the selection process is focused on maximizing the generalization performance of the classifier, that is, the ability of the classifier, designed using the training set, to operate satisfactorily with new data. Among the different design criteria, the maximal margin hyperplane is usually selected since it leaves the maximum margin of separation between the two classes. Since the distance from a point $\mathbf{x}$ to the hyperplane is given by $z=|g(\mathbf{x})| / \|\mathbf{w}\|$, scaling $w$ and $\mathbf{w_0}$ so that the value of $g(\textbf{x})$ is $+1$ for the nearest point in $\omega_1$ and $-1$ for the nearest points in $\omega_2$, reduces the optimization problem to maximizing the margin: $2/\|\mathbf{w}\|$ with the constraints:

\begin{equation}
\mathbf{w}^T \mathbf{x} + \mathbf{w_0} \ge 1, \forall \mathbf{x} \in \omega_1
\end{equation}
\begin{equation}
\mathbf{w}^T \mathbf{x} + \mathbf{w_0} \le 1, \forall \mathbf{x} \in \omega_2
\end{equation}

